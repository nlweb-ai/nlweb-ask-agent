{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison: GPT-4.1 vs Phi-4 Summaries\n",
    "\n",
    "This notebook scores summaries from both models using the Pi Labs API and compares their quality distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "PI_LABS_ENDPOINT = os.environ[\"PI_LABS_ENDPOINT\"]\n",
    "PI_LABS_KEY = os.environ[\"PI_LABS_KEY\"]\n",
    "\n",
    "print(f\"Pi Labs endpoint: {PI_LABS_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summary CSVs\n",
    "df_gpt = pd.read_csv(\"gpt_4_1_summaries.csv\")\n",
    "df_phi = pd.read_csv(\"phi_4_summaries.csv\")\n",
    "\n",
    "# Load sample_100 for ranked_results\n",
    "df_sample = pd.read_csv(\"sample_100.csv\")\n",
    "df_sample[\"ranked_results\"] = df_sample[\"ranked_results\"].apply(json.loads)\n",
    "\n",
    "print(f\"GPT-4.1 summaries: {len(df_gpt)}\")\n",
    "print(f\"Phi-4 summaries: {len(df_phi)}\")\n",
    "print(f\"Sample queries: {len(df_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scoring specification\n",
    "with open(\"scoring_spec.json\") as f:\n",
    "    scoring_spec = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(scoring_spec)} scoring criteria:\")\n",
    "for i, criterion in enumerate(scoring_spec, 1):\n",
    "    print(f\"  {i}. {criterion['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Reconstruction\n",
    "\n",
    "Reconstruct the exact prompt sent to the LLM models during summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"Summarize the following search results in 2-3 sentences, highlighting the key information that answers the user's question: {query}\n",
    "\n",
    "Results:\n",
    "{results}\"\"\"\n",
    "\n",
    "\n",
    "def extract_result_fields(result: dict) -> dict:\n",
    "    \"\"\"Extract name and description from schema.org result data.\"\"\"\n",
    "    content = result.get(\"content\", \"{}\")\n",
    "    if isinstance(content, str):\n",
    "        try:\n",
    "            content = json.loads(content)\n",
    "        except json.JSONDecodeError:\n",
    "            content = {}\n",
    "\n",
    "    # Try various schema.org fields for name\n",
    "    name = (\n",
    "        content.get(\"headline\")\n",
    "        or content.get(\"name\")\n",
    "        or content.get(\"caption\")\n",
    "        or result.get(\"url\", \"Unknown\")\n",
    "    )\n",
    "\n",
    "    # Try various fields for description\n",
    "    description = content.get(\"description\") or content.get(\"articleSection\", \"\") or \"\"\n",
    "    if isinstance(description, list):\n",
    "        description = \", \".join(description)\n",
    "\n",
    "    return {\"name\": name, \"description\": description}\n",
    "\n",
    "\n",
    "def build_llm_input(query_text: str, ranked_results: list) -> str:\n",
    "    \"\"\"Reconstruct the exact prompt sent to the LLM.\"\"\"\n",
    "    raw_results = ranked_results[:3]\n",
    "    results = [extract_result_fields(r) for r in raw_results]\n",
    "    results_text = \"\\n\".join(\n",
    "        f\"{i}. {r['name']}: {r['description']}\" for i, r in enumerate(results, 1)\n",
    "    )\n",
    "    return PROMPT_TEMPLATE.format(query=query_text, results=results_text)\n",
    "\n",
    "\n",
    "# Test with first row\n",
    "print(\"Example LLM input:\")\n",
    "print(\"-\" * 60)\n",
    "print(\n",
    "    build_llm_input(\n",
    "        df_sample.iloc[0][\"query_text\"], df_sample.iloc[0][\"ranked_results\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_summary(llm_input: str, llm_output: str, scoring_spec: list) -> float:\n",
    "    \"\"\"Score a summary using Pi Labs API.\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{PI_LABS_ENDPOINT}invocations\",\n",
    "        headers={\"Authorization\": f\"Bearer {PI_LABS_KEY}\"},\n",
    "        json=[\n",
    "            {\n",
    "                \"llm_input\": llm_input,\n",
    "                \"llm_output\": llm_output,\n",
    "                \"scoring_spec\": scoring_spec,\n",
    "            }\n",
    "        ],\n",
    "        timeout=30,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[0][\"total_score\"]\n",
    "\n",
    "\n",
    "# Test with first GPT summary\n",
    "test_input = build_llm_input(\n",
    "    df_sample.iloc[0][\"query_text\"], df_sample.iloc[0][\"ranked_results\"]\n",
    ")\n",
    "test_output = df_gpt.iloc[0][\"summary\"]\n",
    "test_score = score_summary(test_input, test_output, scoring_spec)\n",
    "print(f\"Test score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score GPT-4.1 Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_scores = []\n",
    "\n",
    "for i in range(len(df_gpt)):\n",
    "    print(f\"Scoring GPT-4.1 {i + 1}/{len(df_gpt)}...\", end=\"\\r\")\n",
    "\n",
    "    llm_input = build_llm_input(\n",
    "        df_sample.iloc[i][\"query_text\"],\n",
    "        df_sample.iloc[i][\"ranked_results\"],\n",
    "    )\n",
    "    llm_output = df_gpt.iloc[i][\"summary\"]\n",
    "\n",
    "    score = score_summary(llm_input, llm_output, scoring_spec)\n",
    "    gpt_scores.append(score)\n",
    "\n",
    "df_gpt[\"score\"] = gpt_scores\n",
    "print(f\"\\nScored {len(gpt_scores)} GPT-4.1 summaries\")\n",
    "print(f\"Mean score: {df_gpt['score'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated GPT CSV\n",
    "df_gpt.to_csv(\"gpt_4_1_summaries.csv\", index=False)\n",
    "print(\"Saved gpt_4_1_summaries.csv with score column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Phi-4 Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_scores = []\n",
    "\n",
    "for i in range(len(df_phi)):\n",
    "    print(f\"Scoring Phi-4 {i + 1}/{len(df_phi)}...\", end=\"\\r\")\n",
    "\n",
    "    llm_input = build_llm_input(\n",
    "        df_sample.iloc[i][\"query_text\"],\n",
    "        df_sample.iloc[i][\"ranked_results\"],\n",
    "    )\n",
    "    llm_output = df_phi.iloc[i][\"summary\"]\n",
    "\n",
    "    score = score_summary(llm_input, llm_output, scoring_spec)\n",
    "    phi_scores.append(score)\n",
    "\n",
    "df_phi[\"score\"] = phi_scores\n",
    "print(f\"\\nScored {len(phi_scores)} Phi-4 summaries\")\n",
    "print(f\"Mean score: {df_phi['score'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated Phi CSV\n",
    "df_phi.to_csv(\"phi_4_summaries.csv\", index=False)\n",
    "print(\"Saved phi_4_summaries.csv with score column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Score Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "stats = pd.DataFrame(\n",
    "    {\n",
    "        \"GPT-4.1\": [\n",
    "            df_gpt[\"score\"].mean(),\n",
    "            df_gpt[\"score\"].median(),\n",
    "            df_gpt[\"score\"].std(),\n",
    "            df_gpt[\"score\"].min(),\n",
    "            df_gpt[\"score\"].max(),\n",
    "        ],\n",
    "        \"Phi-4\": [\n",
    "            df_phi[\"score\"].mean(),\n",
    "            df_phi[\"score\"].median(),\n",
    "            df_phi[\"score\"].std(),\n",
    "            df_phi[\"score\"].min(),\n",
    "            df_phi[\"score\"].max(),\n",
    "        ],\n",
    "    },\n",
    "    index=[\"Mean\", \"Median\", \"Std Dev\", \"Min\", \"Max\"],\n",
    ")\n",
    "\n",
    "print(\"Score Statistics Comparison\")\n",
    "print(\"=\" * 40)\n",
    "print(stats.round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.hist(df_gpt[\"score\"], bins=20, alpha=0.5, label=\"GPT-4.1\", color=\"blue\")\n",
    "ax.hist(df_phi[\"score\"], bins=20, alpha=0.5, label=\"Phi-4\", color=\"orange\")\n",
    "\n",
    "ax.set_xlabel(\"Score\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Summary Quality Scores: GPT-4.1 vs Phi-4\")\n",
    "ax.legend()\n",
    "\n",
    "# Add vertical lines for means\n",
    "ax.axvline(\n",
    "    df_gpt[\"score\"].mean(),\n",
    "    color=\"blue\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    "    label=f\"GPT-4.1 mean: {df_gpt['score'].mean():.2f}\",\n",
    ")\n",
    "ax.axvline(\n",
    "    df_phi[\"score\"].mean(),\n",
    "    color=\"orange\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    "    label=f\"Phi-4 mean: {df_phi['score'].mean():.2f}\",\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
